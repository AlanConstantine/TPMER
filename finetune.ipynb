{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import auc, mean_squared_error\n",
    "from torchmetrics import F1Score\n",
    "from tools import *\n",
    "from CONSTANT import *\n",
    "from models import CNNBiLSTM, CNNTransformer\n",
    "from config import Params\n",
    "from torch.utils.data import (\n",
    "    TensorDataset, DataLoader, SequentialSampler, WeightedRandomSampler)\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56804"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_results = load_dict_model(r'./output/KEC/valence_CTransformer_loso_0.0001_64_32/results.pkl')\n",
    "parse_res(bs_results)\n",
    "# valence CTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9817"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_results = load_dict_model(r'./output/KEC/arousal_CLSTM_loso_0.0001_64_32/results.pkl')\n",
    "parse_res(bs_results)\n",
    "# arousal CLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/HKU956/valence_CTransformer_loso_0.0001_256_32/fold1_checkpoint.pt'\n",
    "\n",
    "args = Params(dataset='KEC', \n",
    "              model='CTransformer',\n",
    "              target='arousal', \n",
    "              debug=False, \n",
    "              fcn_input=12608,\n",
    "              batch_size=64\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2837, 4, 400) (2837, 1) (608, 4, 400) (608, 1)\n"
     ]
    }
   ],
   "source": [
    "spliter = load_model(args.spliter)\n",
    "data = pd.read_pickle(args.data)\n",
    "\n",
    "for i, k in enumerate(spliter[args.valid]):\n",
    "    train_index = k['train_index']\n",
    "    test_index = k['test_index']\n",
    "    break\n",
    "\n",
    "dataprepare = DataPrepare(args,\n",
    "                        target=args.target, data=data, train_index=train_index, test_index=test_index, device=args.device, batch_size=args.batch_size\n",
    "                        )\n",
    "\n",
    "train_dataloader, test_dataloader = dataprepare.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kec_fcn = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(args.fcn_input, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "model = CNNTransformer.CTransformer(args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.fcn = kec_fcn\n",
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss_list = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "    return np.mean(train_loss_list)\n",
    "\n",
    "\n",
    "def eval(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    loss_fn = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target.float())\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "def run(train_loader, val_loader, ckpt_path):\n",
    "    best_score = float('inf')\n",
    "    patience = 25\n",
    "    stop_count = 0\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
    "                                    verbose=True, threshold_mode='rel',\n",
    "                                    cooldown=0, min_lr=0, eps=1e-08\n",
    "                                    )\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss = train(model, args.device, train_loader, optimizer, epoch)\n",
    "        val_loss = eval(model, args.device, val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        print('[Epoch{}] | train_loss:{:.4f} | val_loss:{:.4f} | lr:{:e}'.format(epoch, train_loss, val_loss, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        if val_loss < best_score:\n",
    "            best_score = val_loss\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(\"<<<<<< reach best {0} >>>>>>\".format(val_loss))\n",
    "            stop_count = 0\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(ckpt_path))\n",
    "            stop_count += 1\n",
    "            if stop_count >= patience:\n",
    "                print(\"<<<<<< without improvement in {} epoch, early stopping, best score {:.4f} >>>>>>\".format(patience, best_score))\n",
    "                break\n",
    "        # wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    print('best score', best_score)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run(train_dataloader, test_dataloader, ckpt_path=os.path.join(args.save_path, 'arousal_checkpoint_dp02.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# back to HKU956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/KEC/valence_CTransformer_loso_0.0001_32_32/checkpoint.pt'\n",
    "\n",
    "args = Params(dataset='HKU956', \n",
    "              model='CTransformer',\n",
    "              target='arousal', \n",
    "              debug=False, \n",
    "              fcn_input=12608,\n",
    "              batch_size=64\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18089, 4, 400) (18089, 1) (4638, 4, 400) (4638, 1)\n"
     ]
    }
   ],
   "source": [
    "spliter = load_model(args.spliter)\n",
    "data = pd.read_pickle(args.data)\n",
    "\n",
    "for i, k in enumerate(spliter[args.valid]):\n",
    "    train_index = k['train_index']\n",
    "    test_index = k['test_index']\n",
    "    break\n",
    "\n",
    "dataprepare = DataPrepare(args,\n",
    "                        target=args.target, data=data, train_index=train_index, test_index=test_index, device=args.device, batch_size=args.batch_size\n",
    "                        )\n",
    "\n",
    "train_dataloader, test_dataloader = dataprepare.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kec_fcn = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(args.fcn_input, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "model = CNNTransformer.CTransformer(args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.fcn = kec_fcn\n",
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run(train_dataloader, test_dataloader, ckpt_path=os.path.join(args.save_path, 'pretrain_hku_arousal_pretrain_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss_list = []\n",
    "    train_f1s = []\n",
    "    sig = nn.Sigmoid()\n",
    "    f1_m = F1Score().to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target.float())\n",
    "        # f1 = f1_score(torch.round(sig(output)).cpu().detach().numpy().astype(int), target.cpu().detach().numpy(), average='macro')\n",
    "        f1 = f1_m(torch.round(sig(output)).long(), target).item()\n",
    "        train_f1s.append(f1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "    return np.mean(train_loss_list), np.mean(train_f1s)\n",
    "\n",
    "\n",
    "def bin_eval(model, device, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_f1s = []\n",
    "    sig = nn.Sigmoid()\n",
    "    f1_m = F1Score().to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target.float())\n",
    "            # f1 = f1_score(torch.round(sig(output)).cpu().detach().numpy(), target.cpu().detach().numpy(), average='macro')\n",
    "            # print(torch.round(sig(output)).long(), target)\n",
    "            f1 = f1_m(torch.round(sig(output)).long(), target).item()\n",
    "            val_f1s.append(f1)\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(val_loss), np.mean(val_f1s)\n",
    "\n",
    "def bin_run(model, train_loader, val_loader, ckpt_path):\n",
    "    best_score = -1 * float('inf')\n",
    "    patience = 25\n",
    "    stop_count = 0\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
    "                                    verbose=True, threshold_mode='rel',\n",
    "                                    cooldown=0, min_lr=0, eps=1e-08\n",
    "                                    )\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_loss, train_f1 = bin_train(model, args.device, train_loader, optimizer, epoch)\n",
    "        val_loss, val_f1 = bin_eval(model, args.device, val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        print('[Epoch{}] | train_loss:{:.4f} | val_loss:{:.4f} | train_f1:{:.4f} | val_f1:{:.4f} | lr:{:e}'.format(epoch, train_loss, val_loss, train_f1, val_f1, optimizer.param_groups[0]['lr']))\n",
    "\n",
    "        if val_f1 > best_score:\n",
    "            best_score = val_f1\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(\"<<<<<< reach best {0} >>>>>>\".format(val_f1))\n",
    "            stop_count = 0\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(ckpt_path))\n",
    "            stop_count += 1\n",
    "            if stop_count >= patience:\n",
    "                print(\"<<<<<< without improvement in {} epoch, early stopping, best score {:.4f} >>>>>>\".format(patience, best_score))\n",
    "                break\n",
    "        # wandb.log({'train_loss': train_loss, 'val_loss': val_loss})\n",
    "    print('best score', best_score)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2756, 4, 400) (2756, 1) (689, 4, 400) (689, 1)\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = r'./output/HKU956/valence_CTransformer_loso_0.0001_256_32/fold2_checkpoint.pt'\n",
    "\n",
    "args = Params(dataset='KEC', \n",
    "              model='CTransformer',\n",
    "              target='arousal_label', \n",
    "              debug=False, \n",
    "              fcn_input=12608,\n",
    "              batch_size=64,\n",
    "              valid='cv'\n",
    "              )\n",
    "\n",
    "# valence_label 0.7562\n",
    "\n",
    "spliter = load_model(args.spliter)\n",
    "data = pd.read_pickle(args.data)\n",
    "\n",
    "for i, k in enumerate(spliter[args.valid]):\n",
    "    train_index = k['train_index']\n",
    "    test_index = k['test_index']\n",
    "    break\n",
    "\n",
    "dataprepare = DataPrepare(args,\n",
    "                        target=args.target, data=data, train_index=train_index, test_index=test_index, device=args.device, batch_size=args.batch_size\n",
    "                        )\n",
    "\n",
    "train_dataloader, test_dataloader = dataprepare.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kec_fcn = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(args.fcn_input, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "model = CNNTransformer.CTransformer(args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.fcn = kec_fcn\n",
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:04, 10.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch1] | train_loss:0.6640 | val_loss:0.6094 | train_f1:0.6584 | val_f1:0.7190 | lr:1.000000e-04\n",
      "<<<<<< reach best 0.7189819108356129 >>>>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch2] | train_loss:0.6420 | val_loss:0.6150 | train_f1:0.6808 | val_f1:0.7190 | lr:1.000000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch3] | train_loss:0.6402 | val_loss:0.6139 | train_f1:0.6808 | val_f1:0.7190 | lr:1.000000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch4] | train_loss:0.6393 | val_loss:0.6144 | train_f1:0.6808 | val_f1:0.7190 | lr:1.000000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch5] | train_loss:0.6416 | val_loss:0.6160 | train_f1:0.6808 | val_f1:0.7190 | lr:1.000000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch6] | train_loss:0.6412 | val_loss:0.6153 | train_f1:0.6808 | val_f1:0.7190 | lr:1.000000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-05.\n",
      "[Epoch7] | train_loss:0.6407 | val_loss:0.6162 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch8] | train_loss:0.6335 | val_loss:0.6095 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch9] | train_loss:0.6324 | val_loss:0.6091 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch10] | train_loss:0.6320 | val_loss:0.6094 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch11] | train_loss:0.6322 | val_loss:0.6094 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch12] | train_loss:0.6339 | val_loss:0.6098 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch13] | train_loss:0.6325 | val_loss:0.6094 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch14] | train_loss:0.6357 | val_loss:0.6109 | train_f1:0.6808 | val_f1:0.7190 | lr:5.000000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 2.5000e-05.\n",
      "[Epoch15] | train_loss:0.6337 | val_loss:0.6092 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch16] | train_loss:0.6281 | val_loss:0.6043 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch17] | train_loss:0.6279 | val_loss:0.6042 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch18] | train_loss:0.6291 | val_loss:0.6046 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch19] | train_loss:0.6269 | val_loss:0.6042 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch20] | train_loss:0.6292 | val_loss:0.6042 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch21] | train_loss:0.6297 | val_loss:0.6042 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch22] | train_loss:0.6292 | val_loss:0.6048 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch23] | train_loss:0.6293 | val_loss:0.6046 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch24] | train_loss:0.6278 | val_loss:0.6041 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch25] | train_loss:0.6263 | val_loss:0.6040 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [00:03, 11.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch26] | train_loss:0.6287 | val_loss:0.6047 | train_f1:0.6808 | val_f1:0.7190 | lr:2.500000e-05\n",
      "<<<<<< without improvement in 25 epoch, early stopping, best score 0.7190 >>>>>>\n",
      "best score 0.7189819108356129\n"
     ]
    }
   ],
   "source": [
    "model = bin_run(model, train_dataloader, test_dataloader, 'bin_kec_valence_pretrain_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2b388c6fce79e00fd9c43dd7c300c62775de93114fdc7222b9aeb8ab89a5a93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import auc, mean_squared_error\n",
    "from torchmetrics import F1Score\n",
    "from tools import *\n",
    "from CONSTANT import *\n",
    "from models import CNNBiLSTM, CNNTransformer\n",
    "from config import Params\n",
    "from torch.utils.data import (\n",
    "    TensorDataset, DataLoader, SequentialSampler, WeightedRandomSampler)\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 0] ===============================\n"
     ]
    }
   ],
   "source": [
    "args = Params(debug=True)\n",
    "spliter = load_model(\n",
    "    r'./processed_signal/HKU956/400_4s_step_2s_spliter.pkl')\n",
    "# data = pd.read_pickle(r'./processed_signal/HKU956/400_4s_step_2s.pkl')\n",
    "data = pd.read_csv(r'./processed_signal/HKU956/400_4s_step_2s.csv')\n",
    "for i, k in enumerate(spliter[args.valid]):\n",
    "    args.k = i\n",
    "    print('[Fold {}]'.format(i), '='*31)\n",
    "    train_index = k['train_index']\n",
    "    test_index = k['test_index']\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrepare(object):\n",
    "    def __init__(self, args, target, data, train_index, test_index, device, batch_size=64):\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        X, y = join_signals(data, target=target)\n",
    "        xtrain, ytrain, xtest, ytest = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "\n",
    "        if self.args.debug:\n",
    "            xtrain, ytrain, xtest, ytest = xtrain[:\n",
    "                                                  100], ytrain[:100], xtest[:100], ytest[:100]\n",
    "        print(xtrain.shape, ytrain.shape, xtest.shape, ytest.shape)\n",
    "\n",
    "        xtrain = torch.from_numpy(xtrain).to(torch.float32)\n",
    "        xtest = torch.from_numpy(xtest).to(torch.float32)\n",
    "\n",
    "        self.xtrain, self.xtest = xtrain.to(device), xtest.to(device)\n",
    "\n",
    "        self.xtrain.requires_grad_()\n",
    "        # self.xtest.requires_grad_()\n",
    "\n",
    "        ytrain = torch.from_numpy(ytrain)\n",
    "        ytest = torch.from_numpy(ytest)\n",
    "\n",
    "        self.ytrain, self.ytest = ytrain.to(device), ytest.to(device)\n",
    "\n",
    "        if args.target in ['valence', 'arousal']:\n",
    "            self.ytrain = self.ytrain.to(torch.float32)\n",
    "            self.ytest = self.ytest.to(torch.float32)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_data(self):\n",
    "        train_data = TensorDataset(self.xtrain, self.ytrain)\n",
    "        test_data = TensorDataset(self.xtest, self.ytest)\n",
    "\n",
    "        train_sampler = SequentialSampler(train_data)\n",
    "        train_dataloader = DataLoader(\n",
    "            train_data, sampler=train_sampler, batch_size=self.batch_size, drop_last=False)\n",
    "\n",
    "        test_sampler = SequentialSampler(test_data)\n",
    "        test_dataloader = DataLoader(\n",
    "            test_data, sampler=test_sampler, batch_size=self.batch_size, drop_last=False)\n",
    "\n",
    "        return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss_list = []\n",
    "    correct_list = []\n",
    "    # loss_fn = nn.BCEWithLogitsLoss()\n",
    "    # loss = F.binary_cross_entropy_with_logits(output, target)\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_dataloader)):\n",
    "        data, target = data.to(args.device), target.to(args.device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        pred = pred.float()\n",
    "        target = target.float()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        loss.requires_grad_()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        correct_list.append(pred.eq(target.view_as(pred)).sum().item()/len(target))\n",
    "    return np.mean(train_loss_list), np.mean(correct_list)\n",
    "\n",
    "def eval(model, device, val_dataloader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    f1_ = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            pred = pred.float()\n",
    "            target = target.float()\n",
    "            loss = F.binary_cross_entropy_with_logits(pred, target)\n",
    "            val_loss += loss.item()  # sum up batch loss\n",
    "            # pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            f1_.append(f1_score(target.cpu().numpy(), pred.flatten().cpu().numpy(), average='weighted'))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "    accuracy = correct / len(val_dataloader.dataset)\n",
    "    return val_loss, accuracy, np.mean(f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4, 400) (100, 1) (100, 4, 400) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "dataprepare = DataPrepare(args,\n",
    "            target='valence', data=data, train_index=train_index, test_index=test_index, device=args.device, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = dataprepare.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNBiLSTM.CNNBiLSTM(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    print('Epoch', epoch)\n",
    "    train_loss, train_acc = train(args, model, train_dataloader, optimizer, epoch)\n",
    "    val_loss, val_acc, f1 = eval(model, args.device, test_dataloader)\n",
    "    print('[Epoch{}] | train_loss:{:.4f} | val_loss:{:.4f} | train_acc:{:.4f} | val_acc:{:.4f} | val_f1:{:.4f} | lr:{:e}'.format(epoch, train_loss,\n",
    "            val_loss, train_acc, val_acc, f1, optimizer.param_groups[0]['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(r'./processed_signal/KEmoCon/KEC_400.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3445, 1606)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = join_signals(data, target='arousal_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3445, 4, 400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = r'./output/HKU956/valence_CTransformer_loso_0.0001_256_32/fold1_checkpoint.pt'\n",
    "\n",
    "args = Params(dataset='KEC', \n",
    "            model='CTransformer',\n",
    "                target='arousal', debug=True, fcn_input=12608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNTransformer.CTransformer(args)\n",
    "model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.2, inplace=False)\n",
       "  (1): Linear(in_features=12608, out_features=128, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Dropout(p=0.2, inplace=False)\n",
       "  (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2b388c6fce79e00fd9c43dd7c300c62775de93114fdc7222b9aeb8ab89a5a93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
